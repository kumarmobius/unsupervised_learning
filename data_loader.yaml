name: Data loader v2-unsupervised
inputs:
  - {name: api_url, type: String, description: 'API URL to fetch JSON dataset'}
  - {name: access_token, type: string, description: 'Bearer access token for API auth'}
  - {name: exclude_column, type: String, description: 'Comma-separated list of columns to exclude from the dataset', default: ''}
  - {name: use_column, type: String, description: 'Comma-separated list of columns to include in the dataset (all other columns will be excluded)', default: ''}
outputs:
  - {name: data, type: Dataset}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import pandas as pd
        import numpy as np
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        import logging

        parser = argparse.ArgumentParser()
        parser.add_argument('--api_url', type=str, required=True)
        parser.add_argument('--access_token', type=str, required=True)
        parser.add_argument('--exclude_column', type=str, default='')
        parser.add_argument('--use_column', type=str, default='')
        parser.add_argument('--output_data', type=str, required=True)
        args = parser.parse_args()

        # Read access token
        with open(args.access_token, 'r') as f:
            access_token = f.read().strip()

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("api_pagination_unsupervised")

        session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {access_token}"
        }

        payload = {
            "dbType": "TIDB",
            "entityId": "",
            "entityIds": [],
            "ownedOnly": False,
            "projections": [],
            "filter": {},
            "startTime": 0,
            "endTime": 0
        }

        def extract_records(body):
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                # Try common keys for paginated responses
                for key in ["data", "content", "records", "results", "items"]:
                    if key in body and isinstance(body[key], list):
                        return body[key]
                return [body]
            return []

        def discover_page_metadata(body):
            page_size = None
            total_pages = None
            total_instances = None

            if isinstance(body, dict):
                # Direct keys
                page_size = body.get("pageSize")
                total_pages = body.get("totalPages")
                total_instances = body.get("totalInstances")
                
                # Nested in 'page' object
                if "page" in body and isinstance(body["page"], dict):
                    p = body["page"]
                    page_size = page_size or p.get("pageSize") or p.get("size")
                    total_pages = total_pages or p.get("totalPages")
                    total_instances = total_instances or p.get("totalInstances") or p.get("totalElements")
                
                # Alternative keys
                total_instances = total_instances or body.get("total") or body.get("totalElements")
                
            return page_size, total_pages, total_instances

        def parse_column_list(column_string):
            if not column_string or column_string.strip() == '':
                return []
            
            # Split by comma, strip whitespace, and filter out empty strings
            columns = [col.strip() for col in column_string.split(',')]
            columns = [col for col in columns if col]
            return columns

        # === STEP 1: METADATA DISCOVERY ===
        logger.info("="*80)
        logger.info("STEP 1: METADATA DISCOVERY")
        logger.info("="*80)
        
        base_url = args.api_url.rstrip('/')
        separator = '&' if '?' in base_url else '?'
        
        # Request first page with metadata flag
        initial_size = 2000  # Fixed page size
        meta_url = f"{base_url}{separator}page=0&size={initial_size}&showPageableMetaData=true"
        
        logger.info(f"Requesting metadata from: {meta_url}")
        try:
            resp_meta = session.post(meta_url, headers=headers, json=payload, timeout=30)
            resp_meta.raise_for_status()
            body_meta = resp_meta.json()
            logger.info("✓ Metadata request successful")
        except requests.exceptions.RequestException as e:
            logger.error(f"✗ Metadata request failed: {e}")
            raise

        # Parse pagination info
        detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
        
        logger.info("-" * 80)
        logger.info("PAGINATION METADATA:")
        logger.info(f"  Server Page Size: {detected_page_size}")
        logger.info(f"  Total Pages: {total_pages}")
        logger.info(f"  Total Instances: {total_instances}")
        logger.info("-" * 80)

        # Determine effective page size
        if detected_page_size:
            page_size = min(detected_page_size, initial_size)
            logger.info(f"Using server-provided page size: {page_size}")
        else:
            inferred_records = extract_records(body_meta)
            page_size = len(inferred_records) if inferred_records else initial_size
            logger.info(f"Inferred page size from first response: {page_size}")

        # Calculate total pages if not provided
        if total_pages is None and total_instances:
            total_pages = (total_instances + page_size - 1) // page_size
            logger.info(f"Calculated total pages from total instances: {total_pages}")
        elif total_pages is None:
            # Fallback: will fetch until empty response
            logger.warning("⚠ No pagination metadata available. Will fetch pages until empty.")

        # === STEP 2: PAGINATED DATA COLLECTION ===
        logger.info("")
        logger.info("="*80)
        logger.info("STEP 2: FETCHING PAGINATED DATA")
        logger.info("="*80)
        
        all_records = []
        
        if total_pages is None:
            # Unknown pagination - fetch until empty
            logger.info("Mode: Fetch until empty response")
            page = 0
            consecutive_empty = 0
            max_consecutive_empty = 3
            
            while consecutive_empty < max_consecutive_empty:
                page_url = f"{base_url}{separator}page={page}&size={page_size}&showPageableMetaData=true"
                logger.info(f"Fetching page {page} (unknown total)...")
                
                try:
                    resp_page = session.post(page_url, headers=headers, json=payload, timeout=30)
                    resp_page.raise_for_status()
                    body_page = resp_page.json()
                    records = extract_records(body_page)
                    
                    if not records:
                        consecutive_empty += 1
                        logger.info(f"  ⚠ Empty page {page} (consecutive: {consecutive_empty}/{max_consecutive_empty})")
                        if consecutive_empty >= max_consecutive_empty:
                            logger.info("  Stopping: reached max consecutive empty pages")
                            break
                    else:
                        consecutive_empty = 0
                        all_records.extend(records)
                        logger.info(f"  ✓ Page {page}: Retrieved {len(records)} records (total: {len(all_records)})")
                    
                    page += 1
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"✗ Failed to fetch page {page}: {e}")
                    raise
        else:
            # Known pagination
            logger.info(f"Mode: Fetching {total_pages} pages with size {page_size}")
            logger.info(f"Expected total records: ~{total_instances or total_pages * page_size}")
            
            for page in range(total_pages):
                page_url = f"{base_url}{separator}page={page}&size={page_size}&showPageableMetaData=true"
                logger.info(f"Fetching page {page + 1}/{total_pages}...")
                
                try:
                    resp_page = session.post(page_url, headers=headers, json=payload, timeout=30)
                    resp_page.raise_for_status()
                    body_page = resp_page.json()
                    records = extract_records(body_page)
                    all_records.extend(records)
                    logger.info(f"  ✓ Retrieved {len(records)} records (cumulative: {len(all_records)})")
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"✗ Failed to fetch page {page}: {e}")
                    raise

        logger.info("")
        logger.info("="*80)
        logger.info(f"COLLECTION COMPLETE: {len(all_records)} total records")
        logger.info("="*80)
        
        if not all_records:
            raise ValueError("No records retrieved from API")

        # === STEP 3: DATAFRAME CREATION ===
        logger.info("")
        logger.info("STEP 3: Creating DataFrame...")
        df = pd.DataFrame(all_records)
        logger.info(f"  Original DataFrame shape: {df.shape}")
        logger.info(f"  Original columns: {list(df.columns)}")

        # === STEP 4: COLUMN FILTERING ===
        logger.info("")
        logger.info("STEP 4: Applying column filtering...")
        
        # Parse column lists
        exclude_columns = parse_column_list(args.exclude_column)
        use_columns = parse_column_list(args.use_column)
        
        logger.info(f"  Columns to exclude: {exclude_columns if exclude_columns else 'None'}")
        logger.info(f"  Columns to use: {use_columns if use_columns else 'All columns (excluding excluded ones)'}")
        
        # Validate column lists
        if exclude_columns and use_columns:
            raise ValueError("Cannot specify both 'exclude_column' and 'use_column' parameters. Use only one.")
        
        if exclude_columns:
            # Check if all columns to exclude exist in the dataframe
            missing_exclude = [col for col in exclude_columns if col not in df.columns]
            if missing_exclude:
                logger.warning(f"  ⚠ Columns not found for exclusion: {missing_exclude}")
            
            # Filter out excluded columns
            columns_to_keep = [col for col in df.columns if col not in exclude_columns]
            if not columns_to_keep:
                raise ValueError("All columns would be excluded. Please adjust your exclude_column parameter.")
            
            df = df[columns_to_keep]
            logger.info(f"  ✓ Excluded {len(exclude_columns)} columns")
            logger.info(f"  Remaining columns: {list(df.columns)}")
            
        elif use_columns:
            # Check if all specified use columns exist
            missing_use = [col for col in use_columns if col not in df.columns]
            if missing_use:
                logger.warning(f"  ⚠ Columns not found for inclusion: {missing_use}")
                # Remove missing columns from the list
                use_columns = [col for col in use_columns if col in df.columns]
            
            if not use_columns:
                raise ValueError("None of the specified 'use_column' values exist in the dataset.")
            
            df = df[use_columns]
            logger.info(f"  ✓ Selected {len(use_columns)} columns")
            logger.info(f"  Selected columns: {list(df.columns)}")
        
        else:
            logger.info("  ✓ No column filtering applied, keeping all columns")

        # === STEP 5: SAVE OUTPUT ===
        logger.info("")
        logger.info("STEP 5: Saving data...")
        
        # Create directory if needed
        output_dir = os.path.dirname(args.output_data)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # Save as Parquet (Argo will handle archiving)
        df.to_parquet(args.output_data, index=False)
        logger.info(f"  ✓ Saved to: {args.output_data}")
        logger.info(f"  Format: Parquet")
        logger.info(f"  Size: {os.path.getsize(args.output_data) / (1024*1024):.2f} MB")

        # === FINAL SUMMARY ===
        logger.info("")
        logger.info("="*80)
        logger.info("FINAL DATA SUMMARY")
        logger.info("="*80)
        logger.info(f"Total records fetched: {len(all_records)}")
        logger.info(f"Final dataset shape: {df.shape}")
        logger.info(f"Columns in final dataset: {list(df.columns)}")
        logger.info(f"Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB")
        logger.info("")
        logger.info("First 3 rows:")
        print(df.head(3).to_string())
        logger.info("")
        logger.info("✓ Data fetching and saving completed successfully")

    args:
      - --api_url
      - {inputValue: api_url}
      - --access_token
      - {inputPath: access_token}
      - --exclude_column
      - {inputValue: exclude_column}
      - --use_column
      - {inputValue: use_column}
      - --output_data
      - {outputPath: data}
