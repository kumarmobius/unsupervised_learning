name: Data Cleaning v1.1 - Unsupervised
inputs:
  - {name: in_file, type: Dataset, description: "Path to the dataset file or directory"}
  - {name: missing_threshold, type: Float, description: "Drop columns with missing % above this", optional: true, default: "0.40"}
  - {name: unique_threshold, type: Float, description: "Drop columns with unique % above this (ID columns)", optional: true, default: "0.95"}
  - {name: numeric_detection_threshold, type: Float, description: "Fraction threshold to coerce object to numeric", optional: true, default: "0.6"}
  - {name: preview_rows, type: Integer, description: "Rows to include in preview prints", optional: true, default: "20"}
  - {name: use_column, type: String, description: "Comma-separated list of columns to use (if empty, use all columns)", optional: true, default: ""}
  - {name: exclude_column, type: String, description: "Comma-separated list of columns to exclude", optional: true, default: ""}
  - {name: drop_constant_columns, type: String, description: "Drop columns with only one unique value: 'true' or 'false'", optional: true, default: "true"}
  - {name: handle_missing_strategy, type: String, description: "Strategy for missing values: 'drop_rows', 'keep', 'drop_cols_only'", optional: true, default: "keep"}
outputs:
  - {name: cleaned_data, type: Dataset, description: "Cleaned dataset parquet"}
  - {name: cleaning_metadata, type: Data, description: "JSON metadata with cleaning stats"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:vtest4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, os, sys, json, traceback, re, io, gzip, zipfile
        from datetime import datetime
        import pandas as pd, numpy as np
        from pathlib import Path
        
        # Helper functions
        def ensure_dir_for(p):
            d=os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def is_likely_json(sample_bytes):
            if not sample_bytes: return False
            try: txt = sample_bytes.decode("utf-8", errors="ignore").lstrip()
            except Exception: return False
            if not txt: return False
            if txt[0] in ("{","["): return True
            if "{" in txt or "[" in txt: return True
            return False

        def read_with_pandas(path):
            path = str(path)
            # if directory: prefer files with known dataset extensions, else fallback to largest file
            if os.path.isdir(path):
                entries = [os.path.join(path, f) for f in os.listdir(path) if not f.startswith('.')]
                files = [p for p in entries if os.path.isfile(p)]
                if not files:
                    raise ValueError("No files in dir: " + path)

                # prefer these extensions in this order
                prefer_exts = ['.parquet', '.pq', '.csv', '.json', '.ndjson', '.gz', '.zip',
                               '.xlsx', '.xls', '.feather', '.feather.parquet', '.sav', '.sas7bdat']
                files_by_ext = {ext: [] for ext in prefer_exts}
                for f in files:
                    ext = os.path.splitext(f)[1].lower()
                    if ext in files_by_ext:
                        files_by_ext[ext].append(f)
                chosen = None
                for ext in prefer_exts:
                    if files_by_ext.get(ext):
                        chosen = files_by_ext[ext][0]
                        break
                if not chosen:
                    # fallback to largest file (legacy behavior)
                    chosen = max(files, key=lambda p: os.path.getsize(p))
                path = chosen
                print("[INFO] Selected file from dir:", path)

            if not os.path.exists(path) or not os.path.isfile(path):
                raise ValueError("Input path not found: " + str(path))

            ext = os.path.splitext(path)[1].lower()

            # handle compressed csv/json (.gz) and zip containers
            if ext == ".gz":
                try:
                    with gzip.open(path, "rt", encoding="utf-8", errors="ignore") as fh:
                        sample = fh.read(8192)
                        fh.seek(0)
                        # prefer json if it looks like json lines
                        if sample.lstrip().startswith("{") or "\\n{" in sample:
                            fh.seek(0)
                            return pd.read_json(fh, lines=True)
                        fh.seek(0)
                        return pd.read_csv(fh)
                except Exception:
                    # fallback to binary open + try encodings
                    pass

            if ext == ".zip":
                with zipfile.ZipFile(path, "r") as z:
                    members = [n for n in z.namelist() if not n.endswith("/")]
                    if not members:
                        raise ValueError("ZIP contains no files: " + path)
                    member = max(members, key=lambda n: z.getinfo(n).file_size)
                    with z.open(member) as fh:
                        sample = fh.read(8192)
                        # json?
                        if sample.lstrip().startswith(b"{") or b"\\n{" in sample:
                            with z.open(member) as fh2:
                                return pd.read_json(io.TextIOWrapper(fh2, encoding="utf-8"), lines=True)
                        else:
                            with z.open(member) as fh2:
                                return pd.read_csv(io.TextIOWrapper(fh2, encoding="utf-8"))

            # Excel support
            if ext in (".xlsx", ".xls"):
                try:
                    # prefer openpyxl for xlsx; xlrd may be needed for old xls
                    return pd.read_excel(path)
                except Exception as e:
                    # give more informative error
                    raise ValueError(f"Failed to read Excel file {path}: {e}")

            # parquet
            if ext in (".parquet", ".pq"):
                return pd.read_parquet(path)

            # feather (if available)
            if ext in (".feather",):
                try:
                    return pd.read_feather(path)
                except Exception:
                    pass

            # If extension suggests text-like or unknown: try a sequence of attempts with multiple encodings/parsers
            attempts = []
            # try JSON (lines and normal)
            try:
                attempts.append(("json_lines", lambda: pd.read_json(path, lines=True)))
            except Exception:
                pass
            try:
                attempts.append(("json", lambda: pd.read_json(path)))
            except Exception:
                pass

            # try CSV with several encodings and engine fallback
            encodings = ['utf-8', 'latin1', 'cp1252', 'utf-16']
            for enc in encodings:
                def make_csv_fn(e=enc):
                    return lambda: pd.read_csv(path, encoding=e)
                attempts.append((f"csv_{enc}", make_csv_fn()))

            # csv with python engine (more tolerant)
            for enc in encodings:
                def make_csv_py(e=enc):
                    return lambda: pd.read_csv(path, encoding=e, engine="python", on_bad_lines='skip')
                attempts.append((f"csv_py_{enc}", make_csv_py()))

            # finally try parquet read as last resort
            attempts.append(("parquet_try", lambda: pd.read_parquet(path)))

            last_exc = None
            for name, fn in attempts:
                try:
                    print(f"[INFO] Trying reader: {name}")
                    df = fn()
                    print(f"[INFO] Succeeded with {name}")
                    return df
                except Exception as e:
                    last_exc = e
                    # continue to next attempt

            # If nothing worked, raise clearer error describing attempts
            raise ValueError(f"Unsupported or unreadable format: {path}. Last error: {last_exc}")


        def make_hashable_for_dupes(df):
            df = df.copy()
            for c in df.columns:
                try:
                    if df[c].apply(lambda v: isinstance(v,(list,dict,set))).any():
                        df[c] = df[c].map(lambda v: json.dumps(v, sort_keys=True) if isinstance(v,(list,dict,set)) else v)
                except Exception:
                    df[c] = df[c].astype(str)
            return df

        CURRENCY_SYMBOLS_REGEX = r'[€£¥₹$¢฿₪₩₫₽₺]'
        MULTIPLIER_MAP = {'k':1e3,'m':1e6,'b':1e9,'t':1e12}
        TOKEN_RE = re.compile(r'^\s*(?P<sign>[-+]?)(?P<number>(?:\d{1,3}(?:[,\s]\d{3})+|\d+)(?:[.,]\d+)?)\s*(?P<mult>[kKmMbBtT])?\s*(?P<unit>[A-Za-z%/°µμ²³]*)\s*$')
        
        def parse_alphanumeric_to_numeric(s, decimal_comma=False):
            if pd.isna(s): return np.nan
            orig=str(s).strip()
            if orig=='' or orig.lower() in {'nan','none','null','na'}: return np.nan
            tmp=re.sub(CURRENCY_SYMBOLS_REGEX,'',orig)
            if tmp.strip().endswith('%'):
                try: return float(tmp.strip().rstrip('%').replace(',','').replace(' ',''))/100.0
                except Exception: pass
            tmp=tmp.replace('\u2212','-').replace('\u2013','-').replace('\u2014','-')
            tmp=re.sub(r'[\u00A0\u202F]','',tmp).strip()
            if decimal_comma: tmp=tmp.replace('.','').replace(',','.')
            else: tmp=tmp.replace(',','').replace(' ','')
            try: return float(tmp)
            except Exception: pass
            m=TOKEN_RE.match(tmp)
            if m:
                number=m.group('number'); mult=m.group('mult')
                number_clean=number.replace(',','').replace(' ','')
                try: val=float(number_clean)
                except Exception: return np.nan
                if mult: val*=MULTIPLIER_MAP.get(mult.lower(),1.0)
                return val
            return np.nan

        def convert_object_columns_advanced(df, detect_threshold=0.6, exclude_cols=None):
            df=df.copy(); report={'converted':[],'skipped':[]}
            exclude_cols = set(exclude_cols or [])
            obj_cols=[c for c in df.columns if (df[c].dtype=='object' or str(df[c].dtype).startswith('string'))]
            for col in obj_cols:
                if col in exclude_cols: continue
                ser=df[col].astype(object); total_non_null=ser.notna().sum()
                if total_non_null==0: 
                    report['skipped'].append({'col':col,'parsable_fraction':0.0}); continue
                parsed = ser.map(lambda x: parse_alphanumeric_to_numeric(x))
                frac = parsed.notna().sum()/float(total_non_null)
                if frac>=detect_threshold:
                    df[col+"_orig"]=df[col]; df[col]=parsed
                    report['converted'].append({'col':col,'parsable_fraction':frac})
                else:
                    report['skipped'].append({'col':col,'parsable_fraction':frac})
            return df, report

        def _get_date_granularity(dt_series, col_name):
            col_lower = col_name.lower()
            
            # Check if it has time information (not all midnight)
            valid_dates = dt_series.dropna()
            if len(valid_dates) > 0:
                has_time = (valid_dates.dt.hour != 0).any() or (valid_dates.dt.minute != 0).any()
                if has_time:
                    return 3  # timestamp with time
                
                # Check if day varies (date level) vs all same day (month level)
                has_day_variation = valid_dates.dt.day.nunique() > 1
                if has_day_variation:
                    return 2  # date level
                else:
                    return 1  # month level only
            
            # Fallback to name-based heuristic
            if 'timestamp' in col_lower or 'datetime' in col_lower:
                return 3
            elif 'date' in col_lower:
                return 2
            elif 'month' in col_lower or 'year' in col_lower:
                return 1
            
            return 2  # default to date level

        def _are_dates_duplicate(dt1, dt2):
            # Align by index
            valid_mask = dt1.notna() & dt2.notna()
            if valid_mask.sum() == 0:
                return False
            
            dt1_valid = dt1[valid_mask]
            dt2_valid = dt2[valid_mask]
            
            # Compare year, month, day - consider them duplicates if at least 95% match
            match_rate = (
                (dt1_valid.dt.year == dt2_valid.dt.year) &
                (dt1_valid.dt.month == dt2_valid.dt.month) &
                (dt1_valid.dt.day == dt2_valid.dt.day)
            ).mean()
            
            return match_rate >= 0.95

        def detect_and_deduplicate_dates(df, exclude_cols=None):
            df = df.copy()
            report = {
                'date_columns': [], 
                'skipped': [], 
                'duplicate_dates_dropped': [],
                'patterns_found': {}
            }
            exclude_cols = set(exclude_cols or [])
            cand = [c for c in df.columns if c not in exclude_cols and 
                    (df[c].dtype == 'object' or str(df[c].dtype).startswith('string'))]
            
            # Step 1: Parse all candidate date columns
            parsed_dates = {}
            for col in cand:
                ser = df[col].astype(object)
                sample = ser.dropna().astype(str).head(500)
                if sample.empty: 
                    continue
                
                try:
                    parsed = pd.to_datetime(sample, errors='coerce', infer_datetime_format=True)
                    frac = parsed.notna().mean()
                except Exception:
                    continue
                
                if frac >= 0.6:
                    try:
                        full = pd.to_datetime(ser, errors='coerce', infer_datetime_format=True)
                        
                        if not pd.api.types.is_datetime64_any_dtype(full):
                            report['skipped'].append({'col': col, 'parsable_fraction': frac, 'reason': 'conversion_failed'})
                            continue
                        
                        if full.notna().sum() == 0:
                            report['skipped'].append({'col': col, 'parsable_fraction': frac, 'reason': 'no_valid_dates'})
                            continue
                        
                        parsed_dates[col] = {
                            'datetime': full,
                            'parsable_fraction': frac,
                            'granularity': _get_date_granularity(full, col)
                        }
                        
                    except Exception as e:
                        report['skipped'].append({'col': col, 'parsable_fraction': frac, 'reason': str(e)})
                        continue
            
            if not parsed_dates:
                return df, report
            
            # Step 2: Group duplicate date columns (same date values)
            date_groups = []
            processed = set()
            
            for col1 in parsed_dates.keys():
                if col1 in processed:
                    continue
                
                group = [col1]
                dt1 = parsed_dates[col1]['datetime']
                
                for col2 in parsed_dates.keys():
                    if col2 == col1 or col2 in processed:
                        continue
                    
                    dt2 = parsed_dates[col2]['datetime']
                    
                    # Check if dates are identical (same year, month, day)
                    if _are_dates_duplicate(dt1, dt2):
                        group.append(col2)
                        processed.add(col2)
                
                processed.add(col1)
                date_groups.append(group)
            
            # Step 3: For each group, keep the best column and drop others
            cols_to_drop = []
            for group in date_groups:
                if len(group) > 1:
                    # Sort by granularity (higher = more detailed)
                    best_col = max(group, key=lambda c: parsed_dates[c]['granularity'])
                    duplicates = [c for c in group if c != best_col]
                    cols_to_drop.extend(duplicates)
                    
                    report['duplicate_dates_dropped'].append({
                        'kept': best_col,
                        'dropped': duplicates,
                        'reason': 'identical_date_values'
                    })
                    print(f"[INFO] Date deduplication: keeping '{best_col}', dropping {duplicates}")
            
            # Drop duplicate date columns from dataframe
            if cols_to_drop:
                df = df.drop(columns=cols_to_drop)
                # Also remove from parsed_dates so we don't process them
                for col in cols_to_drop:
                    del parsed_dates[col]
            
            # Step 4: Extract features from remaining unique date columns
            for col, data in parsed_dates.items():
                full = data['datetime']
                frac = data['parsable_fraction']
                
                try:
                    df[col + "_orig"] = df[col]
                    df[col] = full
                    df[col + "_year"] = df[col].dt.year.astype('Int64')
                    df[col + "_month"] = df[col].dt.month.astype('Int64')
                    df[col + "_day"] = df[col].dt.day.astype('Int64')
                    df[col + "_dayofweek"] = df[col].dt.dayofweek.astype('Int64')
                    df[col + "_quarter"] = df[col].dt.quarter.astype('Int64')
                    df[col + "_month_sin"] = np.sin(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_month_cos"] = np.cos(2 * np.pi * df[col].dt.month / 12)
                    df[col + "_day_sin"] = np.sin(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_day_cos"] = np.cos(2 * np.pi * df[col].dt.dayofweek / 7)
                    df[col + "_is_weekend"] = df[col].dt.dayofweek.isin([5, 6]).astype('Int64')
                    df[col + "_days_since_epoch"] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')
                    
                    report['date_columns'].append({'col': col, 'parsable_fraction': frac})
                    print(f"[INFO] Detected datetime in '{col}' ({frac*100:.1f}% valid)")
                except Exception as e:
                    report['skipped'].append({'col': col, 'parsable_fraction': frac, 'reason': str(e)})
                    continue

            return df, report

        # Main execution
        parser=argparse.ArgumentParser()
        parser.add_argument('--in_file', type=str, required=True)
        parser.add_argument('--missing_threshold', type=float, default=0.40)
        parser.add_argument('--unique_threshold', type=float, default=0.95)
        parser.add_argument('--numeric_detection_threshold', type=float, default=0.6)
        parser.add_argument('--preview_rows', type=int, default=20)
        parser.add_argument('--use_column', type=str, default='')
        parser.add_argument('--exclude_column', type=str, default='')
        parser.add_argument('--drop_constant_columns', type=str, default='true')
        parser.add_argument('--handle_missing_strategy', type=str, default='keep')
        parser.add_argument('--cleaned_data', type=str, required=True)
        parser.add_argument('--cleaning_metadata', type=str, required=True)
        args=parser.parse_args()

        try:
            print("="*80)
            print("BRICK 1: DATA CLEANING v2.0 - UNSUPERVISED LEARNING")
            print("="*80)
            
            # Parse boolean
            drop_const = args.drop_constant_columns.lower() in ('true', '1', 'yes')
            
            # Load data
            print("[STEP 1/11] Loading data...")
            df = read_with_pandas(args.in_file)
            print(f"[INFO] Shape: {df.shape}")
            print(f"[INFO] Columns: {list(df.columns)}")
            
            if len(df) == 0:
                print("[ERROR] Empty dataset", file=sys.stderr)
                sys.exit(1)
            
            # Remove duplicates
            print("[STEP 2/11] Removing duplicates...")
            before_rows = len(df)
            df = make_hashable_for_dupes(df).drop_duplicates(keep='first')
            dropped_dupes = before_rows - len(df)
            print(f"[INFO] Removed {dropped_dupes} duplicate rows")
            
            # Apply column filters
            print("[STEP 3/11] Applying column filters...")
            all_columns_before_filter = list(df.columns)
            kept_user_cols = []
            missing_user_cols = []
            excluded_cols = []
            
            # Handle exclusions first
            if args.exclude_column and args.exclude_column.strip():
                to_exclude = [c.strip() for c in args.exclude_column.split(',') if c.strip()]
                excluded_cols = [c for c in to_exclude if c in df.columns]
                missing_excluded = [c for c in to_exclude if c not in df.columns]
                
                if excluded_cols:
                    df = df.drop(columns=excluded_cols)
                    print(f"[INFO] Excluded {len(excluded_cols)} columns: {excluded_cols}")
                if missing_excluded:
                    print(f"[WARNING] Columns to exclude not found: {missing_excluded}")
            
            # Handle inclusions
            if args.use_column and args.use_column.strip():
                to_use = [c.strip() for c in args.use_column.split(',') if c.strip()]
                kept_user_cols = [c for c in to_use if c in df.columns]
                missing_user_cols = [c for c in to_use if c not in df.columns]
                
                if kept_user_cols:
                    df = df[kept_user_cols]
                    print(f"[INFO] Kept {len(kept_user_cols)} user-specified columns")
                    if missing_user_cols:
                        print(f"[WARNING] Columns not found: {missing_user_cols}")
                else:
                    print(f"[ERROR] None of the specified columns found in dataset", file=sys.stderr)
                    sys.exit(1)
            else:
                print("[INFO] No column filtering - using all remaining columns")
            
            # Drop constant columns (if enabled)
            cols_to_drop_constant = []
            if drop_const:
                print(f"[STEP 4/11] Dropping constant columns...")
                for col in df.columns:
                    if df[col].nunique(dropna=False) <= 1:
                        cols_to_drop_constant.append(col)
                if cols_to_drop_constant:
                    print(f"[INFO] Dropping {len(cols_to_drop_constant)} constant columns: {cols_to_drop_constant}")
                    df = df.drop(columns=cols_to_drop_constant)
                else:
                    print("[INFO] No constant columns found")
            else:
                print(f"[STEP 4/11] Skipping constant column removal (disabled)")
            
            # Drop high-missing columns
            print(f"[STEP 5/11] Dropping columns >{args.missing_threshold*100}% missing...")
            missing_report = {}
            cols_to_drop_missing = []
            for col in df.columns:
                missing_pct = df[col].isnull().sum() / len(df) if len(df) > 0 else 0
                missing_report[col] = float(missing_pct)
                if missing_pct > args.missing_threshold:
                    cols_to_drop_missing.append(col)
            if cols_to_drop_missing:
                print(f"[INFO] Dropping {len(cols_to_drop_missing)} high-missing columns")
                df = df.drop(columns=cols_to_drop_missing)
            else:
                print("[INFO] No high-missing columns to drop")
            
            # Drop ID columns
            print(f"[STEP 6/11] Dropping ID columns >{args.unique_threshold*100}% unique...")
            unique_report = {}
            cols_to_drop_unique = []
            for col in df.columns:
                unique_pct = df[col].nunique() / len(df) if len(df) > 0 else 0
                unique_report[col] = float(unique_pct)
                if unique_pct > args.unique_threshold:
                    is_numeric = pd.api.types.is_numeric_dtype(df[col])
                    is_float = pd.api.types.is_float_dtype(df[col])
                    if is_numeric and is_float and len(df[col].dropna()) > 0:
                        std = df[col].std()
                        if std > 0: continue
                    cols_to_drop_unique.append(col)
            if cols_to_drop_unique:
                print(f"[INFO] Dropping {len(cols_to_drop_unique)} ID columns")
                df = df.drop(columns=cols_to_drop_unique)
            else:
                print("[INFO] No ID columns to drop")
            
            # Convert object to numeric
            print(f"[STEP 7/11] Converting object→numeric...")
            df, conv_report = convert_object_columns_advanced(
                df, detect_threshold=args.numeric_detection_threshold, exclude_cols=set()
            )
            print(f"[INFO] Converted {len(conv_report['converted'])} columns")
            
            # Extract dates with deduplication
            print(f"[STEP 8/11] Extracting date features (with deduplication)...")
            df, date_report = detect_and_deduplicate_dates(df, exclude_cols=set())
            print(f"[INFO] Detected {len(date_report['date_columns'])} unique date columns")
            if date_report['duplicate_dates_dropped']:
                print(f"[INFO] Removed {sum(len(d['dropped']) for d in date_report['duplicate_dates_dropped'])} duplicate date columns")
            
            # Drop _orig columns
            print(f"[STEP 9/11] Cleaning up temporary columns...")
            cols_orig = [c for c in df.columns if c.endswith('_orig')]
            if cols_orig:
                df = df.drop(columns=cols_orig)
                print(f"[INFO] Dropped {len(cols_orig)} _orig columns")
            
            # Handle missing values based on strategy
            print(f"[STEP 10/11] Handling missing values (strategy: {args.handle_missing_strategy})...")
            rows_before_missing = len(df)
            
            if args.handle_missing_strategy == 'drop_rows':
                df = df.dropna()
                dropped_rows = rows_before_missing - len(df)
                print(f"[INFO] Dropped {dropped_rows} rows with any missing values")
            elif args.handle_missing_strategy == 'drop_cols_only':
                print("[INFO] Columns already dropped in previous step, keeping all rows")
            else:  # 'keep'
                print("[INFO] Keeping all rows with missing values")
            
            # Final validation
            if len(df) == 0:
                print("[ERROR] No data remaining after cleaning", file=sys.stderr)
                sys.exit(1)
            
            if len(df.columns) == 0:
                print("[ERROR] No columns remaining after cleaning", file=sys.stderr)
                sys.exit(1)
            
            # Save outputs
            print(f"[STEP 11/11] Saving outputs...")
            ensure_dir_for(args.cleaned_data)
            ensure_dir_for(args.cleaning_metadata)
            
            df.to_parquet(args.cleaned_data, index=False)
            
            # Compute data types summary
            dtypes_summary = {}
            for dtype in df.dtypes.unique():
                dtype_cols = df.select_dtypes(include=[dtype]).columns.tolist()
                dtypes_summary[str(dtype)] = {
                    'count': len(dtype_cols),
                    'columns': dtype_cols
                }
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat()+'Z',
                'version': '2.0',
                'mode': 'unsupervised',
                'original_shape': {'rows': before_rows, 'cols': len(all_columns_before_filter)},
                'final_shape': {'rows': len(df), 'cols': len(df.columns)},
                'dropped_duplicates': dropped_dupes,
                'dropped_constant_cols': cols_to_drop_constant,
                'dropped_missing_cols': cols_to_drop_missing,
                'dropped_unique_cols': cols_to_drop_unique,
                'dropped_rows_missing': rows_before_missing - len(df),
                'user_specified_columns': kept_user_cols if kept_user_cols else 'all',
                'excluded_columns': excluded_cols,
                'missing_user_columns': missing_user_cols,
                'all_columns_before_filter': all_columns_before_filter,
                'missing_report': missing_report,
                'unique_report': unique_report,
                'conversion_report': conv_report,
                'date_report': date_report,
                'column_names': list(df.columns),
                'dtypes_summary': dtypes_summary,
                'missing_strategy': args.handle_missing_strategy,
                'settings': {
                    'missing_threshold': args.missing_threshold,
                    'unique_threshold': args.unique_threshold,
                    'numeric_detection_threshold': args.numeric_detection_threshold,
                    'drop_constant_columns': drop_const
                }
            }
            
            with open(args.cleaning_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"{'='*80}")
            print("BRICK 1 COMPLETE")
            print(f"{'='*80}")
            print(f"Final data shape: {df.shape}")
            print(f"Columns: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}")
            print(f"Data types: {df.dtypes.value_counts().to_dict()}")
            print(f"Saved to: {args.cleaned_data}")
            
        except Exception as exc:
            print(f"ERROR: {exc}", file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --in_file
      - {inputPath: in_file}
      - --missing_threshold
      - {inputValue: missing_threshold}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --numeric_detection_threshold
      - {inputValue: numeric_detection_threshold}
      - --preview_rows
      - {inputValue: preview_rows}
      - --use_column
      - {inputValue: use_column}
      - --exclude_column
      - {inputValue: exclude_column}
      - --drop_constant_columns
      - {inputValue: drop_constant_columns}
      - --handle_missing_strategy
      - {inputValue: handle_missing_strategy}
      - --cleaned_data
      - {outputPath: cleaned_data}
      - --cleaning_metadata
      - {outputPath: cleaning_metadata}
